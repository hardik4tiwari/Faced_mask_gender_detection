{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import face_recognition\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model,save_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_net=cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "# face_net=cv2.dnn.readNetFromCaffe('deploy.prototxt', 'weights.caffemodel')\n",
    "genders= ['Male','Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    c=0\n",
    "    # output=r'C:\\Users\\hardi\\VS Code\\ML\\cropped'\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            gender = int(filename.split('_')[1])  # 0 for male, 1 for female\n",
    "            img_path=os.path.join(data_dir,filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (100, 100))\n",
    "            # cv2.imwrite(output+str(f'\\{c} {gender}.jpg'),img)\n",
    "            # c+=1\n",
    "            labels.append(gender)\n",
    "            images.append(img)\n",
    "    # labels=np.array(labels)\n",
    "    # images=np.array(images,dtype=np.float32)/255.0\n",
    "\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_data(data_dir,batch_size,limit):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    i=0\n",
    "    for filename in os.listdir(data_dir):\n",
    "        img=cv2.imread(os.path.join(data_dir,filename))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image:{filename}\")\n",
    "            continue\n",
    "        (h, w) = img.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        face_net.setInput(blob)\n",
    "        detections = face_net.forward()\n",
    "        max_=0\n",
    "        face=None\n",
    "        for j in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, j, 2]\n",
    "            if(confidence>=max_):\n",
    "                max_=confidence\n",
    "                face_box = detections[0, 0, j, 3:7] * np.array([w, h, w, h])\n",
    "                (start_X, start_Y, end_X, end_Y) = face_box.astype(\"int\")\n",
    "                start_X = max(0, start_X)\n",
    "                start_Y = max(0, start_Y)\n",
    "                end_X = min(w, end_X)\n",
    "                end_Y = min(h, end_Y)\n",
    "                face=img[start_Y:end_Y,start_X:end_X]\n",
    "        if max_>0.4 and face is not None:\n",
    "            h,w=face.shape[:2]\n",
    "            face=face[:h//2,:]\n",
    "            face=cv2.resize(face,(100,100))\n",
    "            images.append(face)\n",
    "            gender = int(filename.split('_')[1])  # 0 for male, 1 for female\n",
    "            labels.append(gender)\n",
    "        else:\n",
    "            print(f'No Face Detected in {filename}')\n",
    "        i+=1\n",
    "        if(i%batch_size==0):\n",
    "            print(f'Processed {i} images')\n",
    "            yield np.array(images)/255.0,np.array(labels)\n",
    "            images,labels=[],[]\n",
    "            gc.collect()\n",
    "\n",
    "        if limit and i>=limit:\n",
    "            break\n",
    "    if images:\n",
    "        yield np.array(images)/255.0,np.array(labels)\n",
    "        gc.collect()\n",
    "    # images=np.array(images)/255.0\n",
    "    # labels=np.array(labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\hardi\\AppData\\Local\\Temp\\ipykernel_14352\\4168312264.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  data_dir='.\\dataset'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Face Detected in 105_1_0_20170112213021902.jpg\n",
      "Processed 100 images\n",
      "Processed 200 images\n",
      "Processed 300 images\n",
      "Processed 400 images\n",
      "Processed 500 images\n",
      "Processed 600 images\n",
      "Processed 700 images\n",
      "No Face Detected in 15_1_0_20170109204416506.jpg\n",
      "Processed 800 images\n",
      "Processed 900 images\n",
      "Processed 1000 images\n",
      "Processed 1100 images\n",
      "No Face Detected in 17_1_0_20170109205309078.jpg\n",
      "Processed 1200 images\n",
      "Processed 1300 images\n",
      "Processed 1400 images\n",
      "Processed 1500 images\n",
      "Processed 1600 images\n",
      "No Face Detected in 1_0_0_20170109194225586.jpg\n",
      "Processed 1700 images\n",
      "Processed 1800 images\n",
      "Processed 1900 images\n",
      "No Face Detected in 1_0_2_20161219195311539.jpg\n",
      "Processed 2000 images\n",
      "Processed 2100 images\n",
      "Processed 2200 images\n",
      "Processed 2300 images\n",
      "No Face Detected in 1_1_0_20161219194707290.jpg\n",
      "Processed 2400 images\n",
      "Processed 2500 images\n",
      "No Face Detected in 1_1_2_20161219160842277.jpg\n",
      "Processed 2600 images\n",
      "Processed 2700 images\n",
      "Processed 2800 images\n",
      "Processed 2900 images\n",
      "Processed 3000 images\n",
      "Processed 3100 images\n",
      "Processed 3200 images\n",
      "Processed 3300 images\n",
      "Processed 3400 images\n",
      "Processed 3500 images\n",
      "Processed 3600 images\n",
      "Processed 3700 images\n",
      "Processed 3800 images\n",
      "Processed 3900 images\n",
      "Processed 4000 images\n",
      "Processed 4100 images\n",
      "Processed 4200 images\n",
      "Processed 4300 images\n",
      "Processed 4400 images\n",
      "Processed 4500 images\n",
      "Processed 4600 images\n",
      "Processed 4700 images\n",
      "Processed 4800 images\n",
      "Processed 4900 images\n",
      "Processed 5000 images\n",
      "Processed 5100 images\n",
      "Processed 5200 images\n",
      "Processed 5300 images\n",
      "Processed 5400 images\n",
      "Processed 5500 images\n",
      "Processed 5600 images\n",
      "Processed 5700 images\n",
      "Processed 5800 images\n",
      "Processed 5900 images\n",
      "Processed 6000 images\n",
      "Processed 6100 images\n",
      "Processed 6200 images\n",
      "Processed 6300 images\n",
      "Processed 6400 images\n",
      "Processed 6500 images\n",
      "Processed 6600 images\n",
      "Processed 6700 images\n",
      "Processed 6800 images\n",
      "Processed 6900 images\n",
      "Processed 7000 images\n",
      "Processed 7100 images\n",
      "Processed 7200 images\n",
      "Processed 7300 images\n",
      "Processed 7400 images\n",
      "Processed 7500 images\n",
      "Processed 7600 images\n",
      "Processed 7700 images\n",
      "Processed 7800 images\n",
      "Processed 7900 images\n",
      "Processed 8000 images\n",
      "Processed 8100 images\n",
      "Processed 8200 images\n",
      "Processed 8300 images\n",
      "Processed 8400 images\n",
      "Processed 8500 images\n",
      "Processed 8600 images\n",
      "Processed 8700 images\n",
      "Processed 8800 images\n",
      "Processed 8900 images\n",
      "Processed 9000 images\n",
      "Processed 9100 images\n",
      "Processed 9200 images\n",
      "Processed 9300 images\n",
      "Processed 9400 images\n",
      "Processed 9500 images\n",
      "Processed 9600 images\n",
      "Processed 9700 images\n",
      "Processed 9800 images\n",
      "Processed 9900 images\n",
      "Processed 10000 images\n",
      "Processed 10100 images\n",
      "Processed 10200 images\n",
      "Processed 10300 images\n",
      "Processed 10400 images\n",
      "Processed 10500 images\n",
      "Processed 10600 images\n",
      "Processed 10700 images\n",
      "Processed 10800 images\n",
      "Processed 10900 images\n",
      "Processed 11000 images\n",
      "Processed 11100 images\n",
      "Processed 11200 images\n",
      "Processed 11300 images\n",
      "Processed 11400 images\n",
      "Processed 11500 images\n",
      "Processed 11600 images\n",
      "Processed 11700 images\n",
      "Processed 11800 images\n",
      "Processed 11900 images\n",
      "Processed 12000 images\n",
      "Processed 12100 images\n",
      "Processed 12200 images\n",
      "Processed 12300 images\n",
      "Processed 12400 images\n",
      "Processed 12500 images\n",
      "Processed 12600 images\n",
      "Processed 12700 images\n",
      "Processed 12800 images\n",
      "Processed 12900 images\n",
      "Processed 13000 images\n",
      "Processed 13100 images\n",
      "Processed 13200 images\n",
      "Processed 13300 images\n",
      "Processed 13400 images\n",
      "Processed 13500 images\n",
      "Processed 13600 images\n",
      "Processed 13700 images\n",
      "Processed 13800 images\n",
      "Processed 13900 images\n",
      "Processed 14000 images\n",
      "Processed 14100 images\n",
      "No Face Detected in 36_0_3_20170119191944665.jpg\n",
      "Processed 14200 images\n",
      "Processed 14300 images\n",
      "Processed 14400 images\n",
      "Processed 14500 images\n",
      "Processed 14600 images\n",
      "Processed 14700 images\n",
      "Processed 14800 images\n",
      "Processed 14900 images\n",
      "Processed 15000 images\n",
      "No Face Detected in 39_0_0_20170111201500864.jpg\n",
      "Processed 15100 images\n",
      "Processed 15200 images\n",
      "Processed 15300 images\n",
      "Processed 15400 images\n",
      "Processed 15500 images\n",
      "No Face Detected in 3_1_3_20161219225507686.jpg\n",
      "Processed 15600 images\n",
      "Processed 15700 images\n",
      "Processed 15800 images\n",
      "Processed 15900 images\n",
      "Processed 16000 images\n",
      "Processed 16100 images\n",
      "Processed 16200 images\n",
      "Processed 16300 images\n",
      "Processed 16400 images\n",
      "Processed 16500 images\n",
      "Processed 16600 images\n",
      "Processed 16700 images\n",
      "Processed 16800 images\n",
      "Processed 16900 images\n",
      "Processed 17000 images\n",
      "Processed 17100 images\n",
      "Processed 17200 images\n",
      "Processed 17300 images\n",
      "Processed 17400 images\n",
      "Processed 17500 images\n",
      "Processed 17600 images\n",
      "Processed 17700 images\n",
      "Processed 17800 images\n",
      "Processed 17900 images\n",
      "Processed 18000 images\n",
      "Processed 18100 images\n",
      "Processed 18200 images\n",
      "Processed 18300 images\n",
      "Processed 18400 images\n",
      "Processed 18500 images\n",
      "Processed 18600 images\n",
      "Processed 18700 images\n",
      "Processed 18800 images\n",
      "Processed 18900 images\n",
      "Processed 19000 images\n",
      "Processed 19100 images\n",
      "Processed 19200 images\n",
      "Processed 19300 images\n",
      "Processed 19400 images\n",
      "Processed 19500 images\n",
      "Processed 19600 images\n",
      "Processed 19700 images\n",
      "Processed 19800 images\n",
      "Processed 19900 images\n",
      "Processed 20000 images\n",
      "No Face Detected in 57_1_0_20170110120803050.jpg\n",
      "No Face Detected in 57_1_0_20170110120843929.jpg\n",
      "Processed 20100 images\n",
      "No Face Detected in 58_0_0_20170111205311209.jpg\n",
      "Processed 20200 images\n",
      "Processed 20300 images\n",
      "Processed 20400 images\n",
      "Processed 20500 images\n",
      "Processed 20600 images\n",
      "Processed 20700 images\n",
      "Processed 20800 images\n",
      "Processed 20900 images\n",
      "Processed 21000 images\n",
      "Processed 21100 images\n",
      "Processed 21200 images\n",
      "Processed 21300 images\n",
      "Processed 21400 images\n",
      "Processed 21500 images\n",
      "Processed 21600 images\n",
      "Processed 21700 images\n",
      "Processed 21800 images\n",
      "Processed 21900 images\n",
      "Processed 22000 images\n",
      "Processed 22100 images\n",
      "Processed 22200 images\n",
      "Processed 22300 images\n",
      "Processed 22400 images\n",
      "Processed 22500 images\n",
      "Processed 22600 images\n",
      "Processed 22700 images\n",
      "No Face Detected in 78_0_0_20170111222249342.jpg\n",
      "No Face Detected in 78_0_0_20170111222546535.jpg\n",
      "Processed 22800 images\n",
      "Processed 22900 images\n",
      "Processed 23000 images\n",
      "Processed 23100 images\n",
      "Processed 23200 images\n",
      "No Face Detected in 85_1_0_20170110181621323.jpg\n",
      "Processed 23300 images\n",
      "Processed 23400 images\n",
      "Processed 23500 images\n",
      "Processed 23600 images\n",
      "Processed 23700 images\n",
      "Processed 23800 images\n",
      "Processed 23900 images\n",
      "Processed 24000 images\n"
     ]
    }
   ],
   "source": [
    "data_dir='.\\dataset'\n",
    "\n",
    "images=[]\n",
    "labels=[]\n",
    "\n",
    "for image,label in load_preprocess_data(data_dir,batch_size=100,limit=24000):\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "images=np.vstack(images)\n",
    "labels=np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(images[500], cv2.COLOR_BGR2RGB))\n",
    "# plt.show()\n",
    "X_train, X_val, y_train, y_val = train_test_split(images,labels, test_size=0.2, random_state=42)\n",
    "# print(X_train.shape(),y_train.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hardi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "gender_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(100,100,3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    # BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification: Male (0) or Female (1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 227ms/step - accuracy: 0.5698 - loss: 0.7496 - val_accuracy: 0.7265 - val_loss: 0.5382\n",
      "Epoch 2/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 218ms/step - accuracy: 0.7342 - loss: 0.5263 - val_accuracy: 0.7688 - val_loss: 0.4588\n",
      "Epoch 3/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 216ms/step - accuracy: 0.7779 - loss: 0.4651 - val_accuracy: 0.7930 - val_loss: 0.4278\n",
      "Epoch 4/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 209ms/step - accuracy: 0.7992 - loss: 0.4274 - val_accuracy: 0.8013 - val_loss: 0.4174\n",
      "Epoch 5/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 211ms/step - accuracy: 0.8101 - loss: 0.4058 - val_accuracy: 0.8138 - val_loss: 0.3914\n",
      "Epoch 6/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 208ms/step - accuracy: 0.8227 - loss: 0.3835 - val_accuracy: 0.8174 - val_loss: 0.3832\n",
      "Epoch 7/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 206ms/step - accuracy: 0.8379 - loss: 0.3585 - val_accuracy: 0.8297 - val_loss: 0.3816\n",
      "Epoch 8/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 229ms/step - accuracy: 0.8488 - loss: 0.3408 - val_accuracy: 0.8293 - val_loss: 0.3830\n",
      "Epoch 9/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 224ms/step - accuracy: 0.8564 - loss: 0.3191 - val_accuracy: 0.8276 - val_loss: 0.3796\n",
      "Epoch 10/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 228ms/step - accuracy: 0.8711 - loss: 0.2896 - val_accuracy: 0.8320 - val_loss: 0.3834\n",
      "Epoch 11/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 229ms/step - accuracy: 0.8822 - loss: 0.2792 - val_accuracy: 0.8357 - val_loss: 0.3869\n",
      "Epoch 12/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 227ms/step - accuracy: 0.8915 - loss: 0.2557 - val_accuracy: 0.8366 - val_loss: 0.4057\n",
      "Epoch 13/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 230ms/step - accuracy: 0.8936 - loss: 0.2478 - val_accuracy: 0.8380 - val_loss: 0.4086\n",
      "Epoch 14/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 227ms/step - accuracy: 0.9066 - loss: 0.2226 - val_accuracy: 0.8359 - val_loss: 0.4105\n",
      "Epoch 15/15\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 215ms/step - accuracy: 0.9106 - loss: 0.2079 - val_accuracy: 0.8326 - val_loss: 0.4449\n"
     ]
    }
   ],
   "source": [
    "gender_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "#Model Training\n",
    "# gender_model.fit(datagen.flow(X_train, y_train, batch_size=80), epochs=20, validation_data=(X_val, y_val))\n",
    "gender_model.fit(X_train, y_train, batch_size=100, epochs=15, validation_data=(X_val, y_val))\n",
    "gender_model.save('Gender_Model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model=load_model('Gender_Model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, face_box):\n",
    "    (startX, startY, endX, endY) = face_box\n",
    "    face = image[startY:endY, startX:endX]\n",
    "    \n",
    "    # Extracting eye and forehead regions by assuming that it would occupy the upper half of the face_box\n",
    "    height, width = face.shape[:2]\n",
    "    eyes_region = face[:height//2, :]\n",
    "    # Resizing the eyes_region to match the input size of the gender model\n",
    "    eyes_region_resized = cv2.resize(eyes_region, (100,100))\n",
    "    # plt.imshow(cv2.cvtColor(eyes_region_resized, cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "    \n",
    "    return eyes_region_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(image):\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "    max=0\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if(confidence>=max):\n",
    "            max=confidence\n",
    "            face_box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (start_X, start_Y, end_X, end_Y) = face_box.astype(\"int\")\n",
    "            features = extract_features(image,(start_X, start_Y, end_X, end_Y))\n",
    "            features = np.expand_dims(features, axis=0)  # Add batch dimension for the input to work with neural network\n",
    "            features = features / 255.0  # Normalize pixel values\n",
    "            gender_preds=gender_model.predict(features)   \n",
    "            if(gender_preds[0]<0.5):\n",
    "                gender=genders[0]\n",
    "            else:\n",
    "                gender=genders[1]\n",
    "\n",
    "    return gender_preds,gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "[[0.99629086]] Female\n"
     ]
    }
   ],
   "source": [
    "# image = cv2.imread(r'C:\\Users\\hardi\\VS Code\\ML\\RWMFD_part_2_pro\\00111\\003.jpg')\n",
    "image = cv2.imread(r\"C:\\Users\\hardi\\VS Code\\ML\\WhatsApp Image 2024-05-23 at 09.46.30_11d6bd36.jpg\")\n",
    "x,result = predict_gender(image)\n",
    "print(x,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_accuracy(data_dir):\n",
    "    images=[]\n",
    "    test_label=[]\n",
    "    pred_label=[]\n",
    "    for filename in os.listdir(data_dir):\n",
    "        img=cv2.imread(os.path.join(data_dir,filename))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image:{filename}\")\n",
    "            continue\n",
    "        else:\n",
    "            images.append(img)\n",
    "            gender=int(filename[-5])\n",
    "            test_label.append(gender)\n",
    "    test_label=np.array(test_label)\n",
    "    i=0\n",
    "    cost=0\n",
    "    for img in images:\n",
    "        x,gender=predict_gender(img)\n",
    "        if(gender=='Male'):\n",
    "            pred_label.append(0)\n",
    "        else:\n",
    "            pred_label.append(1)\n",
    "    pred_label=np.array(pred_label)\n",
    "    accuracy = np.mean(pred_label == test_label)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\hardi\\AppData\\Local\\Temp\\ipykernel_14352\\3753744714.py:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  test_dir='.\\Cropped'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Accuracy: 0.6546762589928058\n"
     ]
    }
   ],
   "source": [
    "test_dir='.\\Cropped'\n",
    "print(f'Accuracy: {pred_accuracy(test_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_gen(img):\n",
    "    x,result = predict_gender(img)\n",
    "    return result\n",
    "\n",
    "interface = gr.Interface(fn=predict_gen, inputs=\"image\", outputs=\"text\")\n",
    "interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
